{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import psutil\n",
    "\n",
    "# Function to print memory usage\n",
    "def print_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    mem_info = process.memory_info()\n",
    "    print(f\"Memory Usage: {mem_info.rss / (1024 ** 2):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Multiply two large matrices\n",
    "# FAILED if matrices too large and memory is not enough\n",
    "\n",
    "A = np.random.rand(100000, 1)\n",
    "B = np.random.rand(1, 100000)\n",
    "\n",
    "# Standard multiplication\n",
    "C = np.dot(A, B)  # Or A @ B\n",
    "\n",
    "# Print the shape of the result\n",
    "print(\"Shape of C:\", C.shape)\n",
    "\n",
    "# Optionally, print a small part of the matrix\n",
    "print(\"First 5x5 block of C:\\n\", C[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before multiplication:\n",
      "Memory Usage: 2582.28 MB\n",
      "Memory usage after multiplication:\n",
      "Memory Usage: 3534.44 MB\n",
      "Shape of C: (100000, 100000)\n",
      "First 5x5 block of C:\n",
      " [[0.02896208 0.00425345 0.01149264 0.0286885  0.00942824]\n",
      " [0.44973823 0.06604978 0.17846373 0.44548994 0.14640653]\n",
      " [0.10924079 0.0160434  0.0433486  0.10820889 0.03556195]\n",
      " [0.590646   0.08674388 0.23437834 0.58506674 0.19227727]\n",
      " [0.19842781 0.02914165 0.07873951 0.19655344 0.06459564]]\n"
     ]
    }
   ],
   "source": [
    "# Use numpy.memmap for Disk-Based Computation\n",
    "# PERFECT METHOD FOR LOW MEMORY SYSTEMS\n",
    "import numpy as np\n",
    "\n",
    "print(\"Memory usage before multiplication:\")\n",
    "print_memory_usage()\n",
    "\n",
    "A = np.memmap('A.dat', dtype='float32', mode='w+', shape=(100000, 1))\n",
    "B = np.memmap('B.dat', dtype='float32', mode='w+', shape=(1, 100000))\n",
    "C = np.memmap('C.dat', dtype='float32', mode='w+', shape=(100000, 100000))\n",
    "\n",
    "# Fill A and B with random values (example)\n",
    "A[:] = np.random.rand(100000, 1)\n",
    "B[:] = np.random.rand(1, 100000)\n",
    "\n",
    "# Multiply in chunks\n",
    "for i in range(A.shape[0]):\n",
    "    C[i, :] = A[i, :] @ B  # Process row-by-row\n",
    "\n",
    "# Check memory usage after multiplication\n",
    "print(\"Memory usage after multiplication:\")\n",
    "print_memory_usage()\n",
    "\n",
    "# # Print the shape of the result\n",
    "print(\"Shape of C:\", C.shape)\n",
    "\n",
    "# # Optionally, print a small part of the matrix\n",
    "print(\"First 5x5 block of C:\\n\", C[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before multiplication:\n",
      "Memory Usage: 224.27 MB\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Invalid buffer size: 37.25 GB",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m A \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m100000\u001b[39m, \u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     12\u001b[0m B \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100000\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 14\u001b[0m C \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# GPU-accelerated matrix multiplication\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Check memory usage after multiplication\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMemory usage after multiplication:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Invalid buffer size: 37.25 GB"
     ]
    }
   ],
   "source": [
    "# Matrix Multiplication with MPS\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"Memory usage before multiplication:\")\n",
    "print_memory_usage()\n",
    "\n",
    "# Move tensors to GPU (MPS)\n",
    "device = torch.device(\"mps\")  # Use \"cpu\" if no GPU is available\n",
    "\n",
    "A = torch.rand(100000, 1, device=device)\n",
    "B = torch.rand(1, 100000, device=device)\n",
    "\n",
    "C = torch.matmul(A, B)  # GPU-accelerated matrix multiplication\n",
    "\n",
    "# Check memory usage after multiplication\n",
    "print(\"Memory usage after multiplication:\")\n",
    "print_memory_usage()\n",
    "\n",
    "# # Print the shape of the result\n",
    "print(\"Shape of C:\", C.shape)\n",
    "\n",
    "# # Optionally, print a small part of the matrix\n",
    "print(\"First 5x5 block of C:\\n\", C[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cupy\n",
      "  Using cached cupy-13.3.0.tar.gz (3.4 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[3 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Generating cache key from header files...\n",
      "  \u001b[31m   \u001b[0m Cache key (1610 files matching /private/var/folders/yr/qjfqfcxs79b91s4n4v1zp0b80000gn/T/pip-install-e_73c8eh/cupy_cc40364dd651444d9dd3518dbe0e7d23/cupy/_core/include/**): 784b76636589cd680d6706d325ea744f1b389695\n",
      "  \u001b[31m   \u001b[0m Error: macOS is no longer supported\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cupy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use GPU Acceleration (cupy or torch.cuda)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# If you have a GPU, cupy or torch.cuda can handle large matrices efficiently.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m A \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m100000\u001b[39m, \u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m      7\u001b[0m B \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100000\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cupy'"
     ]
    }
   ],
   "source": [
    "# Use GPU Acceleration (cupy or torch.cuda)\n",
    "# If you have a GPU, cupy or torch.cuda can handle large matrices efficiently.\n",
    "# CANNOT BE USED IN MACOSX\n",
    "\n",
    "import cupy as cp\n",
    "\n",
    "A = cp.random.rand(100000, 50)\n",
    "B = cp.random.rand(50, 100000)\n",
    "\n",
    "C = A @ B  # Performed on GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Use scipy.sparse for Sparse Matrices\n",
    "# If your matrices contain many zeros, use scipy.sparse:\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "A = csr_matrix(np.random.rand(100000, 50))\n",
    "B = csr_matrix(np.random.rand(50, 100000))\n",
    "\n",
    "C = A @ B  # Sparse multiplication, saving memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before multiplication:\n",
      "Memory Usage: 76.23 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Python_3-9-19/lib/python3.9/site-packages/dask/array/routines.py:450: PerformanceWarning: Increasing number of chunks by factor of 100\n",
      "  out = blockwise(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Use dask for Out-of-Core Computation\n",
    "# dask.array allows parallel and chunk-based computations.\n",
    "\n",
    "import dask.array as da\n",
    "\n",
    "# Check memory usage after multiplication\n",
    "print(\"Memory usage before multiplication:\")\n",
    "print_memory_usage()\n",
    "\n",
    "A = da.random.random((100000, 1), chunks=(1000, 1))\n",
    "B = da.random.random((1, 100000), chunks=(1, 1000))\n",
    "\n",
    "C = A @ B  # Computation is lazy, not in memory yet\n",
    "C.compute()  # Compute when needed\n",
    "\n",
    "# Check memory usage after multiplication\n",
    "print(\"Memory usage after multiplication:\")\n",
    "print_memory_usage()\n",
    "\n",
    "# # Print the shape of the result\n",
    "print(\"Shape of C:\", C.shape)\n",
    "\n",
    "# # Optionally, print a small part of the matrix\n",
    "print(\"First 5x5 block of C:\\n\", C[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "A = jnp.ones((100000, 1))\n",
    "B = jnp.ones((1, 100000))\n",
    "\n",
    "C = jnp.dot(A, B)  # Automatically optimized\n",
    "\n",
    "# # Print the shape of the result\n",
    "print(\"Shape of C:\", C.shape)\n",
    "\n",
    "# # Optionally, print a small part of the matrix\n",
    "print(\"First 5x5 block of C:\\n\", C[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "A = tf.random.normal([100000, 1])\n",
    "B = tf.random.normal([1, 1000000])\n",
    "\n",
    "C = tf.linalg.matmul(A, B)  # GPU-accelerated\n",
    "\n",
    "# # Print the shape of the result\n",
    "print(\"Shape of C:\", C.shape)\n",
    "\n",
    "# # Optionally, print a small part of the matrix\n",
    "print(\"First 5x5 block of C:\\n\", C[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyopencl as cl\n",
    "import numpy as np\n",
    "\n",
    "ctx = cl.create_some_context()\n",
    "queue = cl.CommandQueue(ctx)\n",
    "\n",
    "A = np.random.rand(10000, 5000).astype(np.float32)\n",
    "B = np.random.rand(5000, 8000).astype(np.float32)\n",
    "\n",
    "# Transfer data to the GPU\n",
    "A_buf = cl.Buffer(ctx, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=A)\n",
    "B_buf = cl.Buffer(ctx, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=B)\n",
    "C_buf = cl.Buffer(ctx, cl.mem_flags.WRITE_ONLY, A.nbytes * B.shape[1])\n",
    "\n",
    "# OpenCL kernel for matrix multiplication (requires setup)\n",
    "# You'd need to write a kernel function to compute the matrix product.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_3-9-19",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
